{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Install and import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: momaland in ./.venv/lib/python3.11/site-packages (0.0.2)\n",
      "Requirement already satisfied: gymnasium>=0.28 in ./.venv/lib/python3.11/site-packages (from momaland) (0.29.1)\n",
      "Requirement already satisfied: pettingzoo[butterfly,sisl] in ./.venv/lib/python3.11/site-packages (from momaland) (1.24.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in ./.venv/lib/python3.11/site-packages (from momaland) (1.26.2)\n",
      "Requirement already satisfied: networkx>=3.1 in ./.venv/lib/python3.11/site-packages (from momaland) (3.2.1)\n",
      "Requirement already satisfied: sympy>=1.12 in ./.venv/lib/python3.11/site-packages (from momaland) (1.12)\n",
      "Requirement already satisfied: pygame>=2.0.1 in ./.venv/lib/python3.11/site-packages (from momaland) (2.3.0)\n",
      "Requirement already satisfied: PyOpenGL==3.1.6 in ./.venv/lib/python3.11/site-packages (from momaland) (3.1.6)\n",
      "Requirement already satisfied: PyOpenGL-accelerate>=3.1.1 in ./.venv/lib/python3.11/site-packages (from momaland) (3.1.7)\n",
      "Requirement already satisfied: pillow>=8.3.1 in ./.venv/lib/python3.11/site-packages (from momaland) (10.1.0)\n",
      "Requirement already satisfied: wandb>=0.16.1 in ./.venv/lib/python3.11/site-packages (from momaland) (0.16.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in ./.venv/lib/python3.11/site-packages (from gymnasium>=0.28->momaland) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in ./.venv/lib/python3.11/site-packages (from gymnasium>=0.28->momaland) (4.8.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in ./.venv/lib/python3.11/site-packages (from gymnasium>=0.28->momaland) (0.0.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./.venv/lib/python3.11/site-packages (from sympy>=1.12->momaland) (1.3.0)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in ./.venv/lib/python3.11/site-packages (from wandb>=0.16.1->momaland) (8.1.7)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in ./.venv/lib/python3.11/site-packages (from wandb>=0.16.1->momaland) (3.1.40)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in ./.venv/lib/python3.11/site-packages (from wandb>=0.16.1->momaland) (2.31.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in ./.venv/lib/python3.11/site-packages (from wandb>=0.16.1->momaland) (5.9.7)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in ./.venv/lib/python3.11/site-packages (from wandb>=0.16.1->momaland) (1.39.1)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in ./.venv/lib/python3.11/site-packages (from wandb>=0.16.1->momaland) (0.4.0)\n",
      "Requirement already satisfied: PyYAML in ./.venv/lib/python3.11/site-packages (from wandb>=0.16.1->momaland) (6.0.1)\n",
      "Requirement already satisfied: setproctitle in ./.venv/lib/python3.11/site-packages (from wandb>=0.16.1->momaland) (1.3.3)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.11/site-packages (from wandb>=0.16.1->momaland) (69.0.3)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in ./.venv/lib/python3.11/site-packages (from wandb>=0.16.1->momaland) (1.4.4)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in ./.venv/lib/python3.11/site-packages (from wandb>=0.16.1->momaland) (4.25.1)\n",
      "Requirement already satisfied: pymunk==6.2.0 in ./.venv/lib/python3.11/site-packages (from pettingzoo[butterfly,sisl]->momaland) (6.2.0)\n",
      "Requirement already satisfied: box2d-py==2.3.5 in ./.venv/lib/python3.11/site-packages (from pettingzoo[butterfly,sisl]->momaland) (2.3.5)\n",
      "Requirement already satisfied: scipy>=1.4.1 in ./.venv/lib/python3.11/site-packages (from pettingzoo[butterfly,sisl]->momaland) (1.11.3)\n",
      "Requirement already satisfied: cffi>1.14.0 in ./.venv/lib/python3.11/site-packages (from pymunk==6.2.0->pettingzoo[butterfly,sisl]->momaland) (1.16.0)\n",
      "Requirement already satisfied: six>=1.4.0 in ./.venv/lib/python3.11/site-packages (from docker-pycreds>=0.4.0->wandb>=0.16.1->momaland) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in ./.venv/lib/python3.11/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb>=0.16.1->momaland) (4.0.11)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb>=0.16.1->momaland) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb>=0.16.1->momaland) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb>=0.16.1->momaland) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb>=0.16.1->momaland) (2023.11.17)\n",
      "Requirement already satisfied: pycparser in ./.venv/lib/python3.11/site-packages (from cffi>1.14.0->pymunk==6.2.0->pettingzoo[butterfly,sisl]->momaland) (2.21)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in ./.venv/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb>=0.16.1->momaland) (5.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install momaland"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create an environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from momaland.envs.multiwalker_stability import momultiwalker_stability_v0\n",
    "import numpy as np\n",
    "\n",
    "env=momultiwalker_stability_v0.env()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Extract environment information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'walker_0': Box(-inf, inf, (31,), float32),\n",
       " 'walker_1': Box(-inf, inf, (31,), float32),\n",
       " 'walker_2': Box(-inf, inf, (31,), float32)}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'walker_0': Box(-1.0, 1.0, (4,), float32),\n",
       " 'walker_1': Box(-1.0, 1.0, (4,), float32),\n",
       " 'walker_2': Box(-1.0, 1.0, (4,), float32)}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'walker_0': Box([  -0.46666667 -110.         -100.        ], [0.46666667 0.         0.        ], (3,), float32),\n",
       " 'walker_1': Box([  -0.46666667 -110.         -100.        ], [0.46666667 0.         0.        ], (3,), float32),\n",
       " 'walker_2': Box([  -0.46666667 -110.         -100.        ], [0.46666667 0.         0.        ], (3,), float32)}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reward_spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.1: AEC API Demo\n",
    "Observation, rewards, termination, truncation, and info are returned by the `last()` function, as in PZ. Except the rewards are vectorial!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0., 0., 0.], dtype=float32),\n",
       " array([0., 0., 0.], dtype=float32),\n",
       " array([0., 0., 0.], dtype=float32)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()\n",
    "episode_rewards = []\n",
    "for agent in env.agent_iter():\n",
    "    # the rewards are vectors!\n",
    "    observation, vec_reward, termination, truncation, info = env.last()\n",
    "    episode_rewards.append(vec_reward)\n",
    "    if termination or truncation:\n",
    "        action = None\n",
    "    else:\n",
    "        action = env.action_space(agent).sample()\n",
    "    env.step(action)\n",
    "env.close()\n",
    "\n",
    "# rewards of all agents from the first step\n",
    "episode_rewards[0:len(env.possible_agents)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.2: Parallel API Demo\n",
    "The environment is initialized with the `parallel_env()` function. Agents `step()` all at the same time through the environment with their actions. A key difference between AEC and Parallel is that actions and observations are dictionaries in Parallel, as they are all received at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'agent_0': array([0., 0., 0.]), 'agent_1': array([0., 0., 0.])}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from momaland.envs.item_gathering import moitem_gathering_v0\n",
    "\n",
    "# new parallel env\n",
    "env = moitem_gathering_v0.parallel_env()\n",
    "observations, infos = env.reset()\n",
    "episode_rewards = []\n",
    "while env.agents:\n",
    "    actions = {agent: env.action_space(agent).sample() for agent in env.agents}\n",
    "    observations, vec_rewards, terminations, truncations, infos = env.step(actions)\n",
    "    episode_rewards.append(vec_rewards)\n",
    "env.close()\n",
    "\n",
    "# rewards are stored in a dictionary, can be accessed per agent\n",
    "episode_rewards[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Wrappers\n",
    "On top of the native wrappers provided by MOMAland; SuperSuit and PettingZoo wrappers are also compatible with MOMAland environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MOMAland"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1.7753998190722102, -0.9956741660833359, -6.084330405294895]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from momaland.utils.aec_wrappers import LinearizeReward, NormalizeReward\n",
    "import numpy as np\n",
    "\n",
    "env = momultiwalker_stability_v0.env()\n",
    "\n",
    "# Normalizing the reward of each agent\n",
    "for agent in env.possible_agents:\n",
    "    for idx in range(env.reward_space(agent).shape[0]):\n",
    "        env = NormalizeReward(env, agent, idx)\n",
    "\n",
    "# Making the vector reward a scalar reward to shift to single-objective multi-agent (aka PettingZoo)\n",
    "# We can assign different weights to the objectives of each agent.\n",
    "weights = {\n",
    "    \"walker_0\": np.array([0.7, 0.3]), \n",
    "    \"walker_1\": np.array([0.5, 0.5]), \n",
    "    \"walker_2\": np.array([0.2, 0.8]),\n",
    "}\n",
    "env = LinearizeReward(env, weights)\n",
    "\n",
    "# Now we are dealing with a regular PZ env\n",
    "env.reset()\n",
    "episode_rewards = []\n",
    "for agent in env.agent_iter():\n",
    "    observation, reward, termination, truncation, info = env.last()\n",
    "    episode_rewards.append(reward)\n",
    "    if termination or truncation:\n",
    "        action = None\n",
    "    else:\n",
    "        action = env.action_space(agent).sample()\n",
    "    env.step(action)\n",
    "env.close()\n",
    "\n",
    "# scalarized and normalized rewards of all agents from the last step\n",
    "episode_rewards.reverse()\n",
    "episode_rewards[0:len(env.possible_agents)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SuperSuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'agent_0': array([0.48762643, 0.48954174, 0.28093657, 0.66940665, 0.6666667 ,\n",
       "        0.86621314, 0.63619226, 0.6529357 , 0.3703892 , 0.5159465 ,\n",
       "        0.66146713, 0.39128128, 0.8341556 , 0.8615172 , 0.332849  ,\n",
       "        1.        , 0.        , 0.        , 0.        ], dtype=float32),\n",
       " 'agent_1': array([0.63619226, 0.6529357 , 0.3703892 , 0.66940665, 0.6666667 ,\n",
       "        0.86621314, 0.48762643, 0.48954174, 0.28093657, 0.5159465 ,\n",
       "        0.66146713, 0.39128128, 0.8341556 , 0.8615172 , 0.332849  ,\n",
       "        0.        , 1.        , 0.        , 0.        ], dtype=float32),\n",
       " 'agent_2': array([0.5159465 , 0.66146713, 0.39128128, 0.66940665, 0.6666667 ,\n",
       "        0.86621314, 0.48762643, 0.48954174, 0.28093657, 0.63619226,\n",
       "        0.6529357 , 0.3703892 , 0.8341556 , 0.8615172 , 0.332849  ,\n",
       "        0.        , 0.        , 1.        , 0.        ], dtype=float32),\n",
       " 'agent_3': array([0.8341556 , 0.8615172 , 0.332849  , 0.66940665, 0.6666667 ,\n",
       "        0.86621314, 0.48762643, 0.48954174, 0.28093657, 0.63619226,\n",
       "        0.6529357 , 0.3703892 , 0.5159465 , 0.66146713, 0.39128128,\n",
       "        0.        , 0.        , 0.        , 1.        ], dtype=float32)}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from supersuit import clip_actions_v0, normalize_obs_v0, agent_indicator_v0\n",
    "from momaland.envs.crazyrl.catch import catch_v0\n",
    "# Parallel SS wrappers\n",
    "env = catch_v0.parallel_env()\n",
    "env = clip_actions_v0(env)\n",
    "env = normalize_obs_v0(env)\n",
    "env = agent_indicator_v0(env)\n",
    "\n",
    "observations, infos = env.reset()\n",
    "actions = {agent: env.action_space(agent).sample() for agent in env.agents}\n",
    "observations, vec_rewards, terminations, truncations, infos = env.step(actions)\n",
    "\n",
    "# normalized observation\n",
    "observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PettingZoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AEC PZ wrappers\n",
    "from pettingzoo.utils.wrappers.clip_out_of_bounds import ClipOutOfBoundsWrapper\n",
    "env = momultiwalker_stability_v0.env()\n",
    "env = ClipOutOfBoundsWrapper(env)\n",
    "\n",
    "env.reset()\n",
    "for agent in env.agent_iter():\n",
    "    # the rewards are vectors!\n",
    "    observation, vec_reward, termination, truncation, info = env.last()\n",
    "    if termination or truncation:\n",
    "        action = None\n",
    "    else:\n",
    "        action = env.action_space(agent).sample()\n",
    "    env.step(action)\n",
    "env.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
